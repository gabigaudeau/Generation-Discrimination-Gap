from datasets import load_dataset
import random
from RiddleDataset import RiddleSenseDataset
from transformers import GPTNeoXForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
import torch


def get_first_new_token(output, prompt):
    """Return the first nonempty token generated by the model (different from the prompt)."""
    split_prompt = prompt.split()
    split_output = output.split()

    index = len(split_prompt)
    while index < len(split_output):
        token = split_output[index]
        if token != "" and token is not None:
            return token
        index += 1

    return None


if __name__ == '__main__':
    RANDOM_SEED = 42
    BATCH_SIZE = 64
    K = 5
    DO_SAMPLE = K != 1
    MAX_SEQUENCE_LENGTH = 160

    # Setting the random seed.
    random.seed(RANDOM_SEED)
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

    print("---- PREPARING DATASET ----\n")
    # TODO. Do I need to do any pre-processing of the dataset?
    original_dataset = load_dataset('riddle_sense')

    print("\n---- GENERATION ----\n")
    SIZE = '70m'
    MODEL_NAME = f'EleutherAI/pythia-{SIZE}-deduped'
    REVISION = 'step3000'
    CACHE_DIR = f'./pythia-{SIZE}-deduped/step3000'

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        revision=REVISION,
        cache_dir=CACHE_DIR,
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'

    model = GPTNeoXForCausalLM.from_pretrained(
        MODEL_NAME,
        revision=REVISION,
        cache_dir=CACHE_DIR
    )
    model.to(device)

    valid_set = RiddleSenseDataset(original_dataset['validation'], tokenizer, MAX_SEQUENCE_LENGTH, is_generation=True)
    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE)

    #
    #
    #
    # EXACT MATCH.
    number_of_batches = len(valid_loader)
    processed_batches = 0
    total_score = 0
    for input_ids, answers, labels, prompts in iter(valid_loader):
        processed_batches += 1
        if processed_batches % 10 == 0:
            print(f'Processed {processed_batches}/{number_of_batches} batches.')

        input_ids = input_ids.squeeze(1).to(device)
        outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, num_return_sequences=K,
                                 do_sample=DO_SAMPLE, max_new_tokens=MAX_SEQUENCE_LENGTH + 20)
        tokens = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        index = 0
        for sample in range(BATCH_SIZE):
            sample_score = 0
            for k in range(K):
                token = get_first_new_token(tokens[index], prompts[sample])
                if token.lower() in answers[sample]:
                    sample_score += 1
                index += 1
            total_score += sample_score / K

    print(f"Exact match accuracy: {total_score / number_of_batches}")

    #
    #
    #
    # LOG PROB
    # number_of_batches = len(valid_loader)
    # processed_batches = 0
    # total_score = 0
    # for input_ids, answers, labels, prompts in iter(valid_loader):
    #     processed_batches += 1
    #     if processed_batches % 50 == 0:
    #         print(f'Processed {processed_batches}/{number_of_batches} batches.')
    #
    #     input_ids = input_ids.squeeze(1).to(device)
    #     outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, num_return_sequences=K,
    #                              do_sample=DO_SAMPLE, max_new_tokens=MAX_SEQUENCE_LENGTH + 20,
    #                              return_dict_in_generate=True, output_scores=True, )
    #     tokens = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    #     gen_sequences = outputs.sequences[:, input_ids.shape[-1]:]
    #     scores = torch.stack(outputs.scores, dim=1).softmax(-1)
    #     gen_probs = torch.gather(scores, 2, gen_sequences[:, :, None]).squeeze(-1)
    #
    #     log_prob = 0
    #     for k in range(K):
    #         for token, p in zip(input_ids[0], gen_probs[k]):
    #             if token not in tokenizer.all_special_ids:
    #                 # Deal with multi-word/long tokens.
    #                 # Assumption: sum the probabilities of the parts.
    #                 # https://stackoverflow.com/questions/59435020/get-probability-of-multi-token-word-in-mask-position
    #                 if tokenizer.decode(token) in answer:
    #                     log_prob += p.item()
    #
    #
    #     index = 0
    #     for sample in range(BATCH_SIZE):
    #         sample_score = 0
    #         for k in range(K):
    #             token = get_first_new_token(outputs[index], prompts[sample])
    #             if token.lower() in answers[sample]:
    #                 sample_score += 1
    #             index += 1
    #         total_score += sample_score / K
    #
    # print(f"Exact match accuracy: {total_score / number_of_batches}")
