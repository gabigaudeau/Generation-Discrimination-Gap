from datasets import load_dataset
import random
from transformers import GPTNeoXForCausalLM, AutoTokenizer, set_seed
from torch.utils.data import DataLoader
import torch
import numpy as np
import os
import sys
from torch.utils.data import Dataset
from itertools import islice


class RiddleSenseDataset(Dataset):
    DISCRIMINATION_PROMPT = "[QUESTION]\nIs [ANSWER] a correct answer to the riddle?\n[OUTPUT]"
    GENERATION_PROMPT = "[QUESTION]\nThe answer to the riddle is: [ANSWER]"

    def __init__(self, data, tokenizer, max_len, is_generation, is_exact_match):
        self.data = []
        for entry in data:
            question = entry['question']
            for i in range(len(entry['choices']['text'])):
                label = entry['choices']['label'][i]
                answer = entry['choices']['text'][i]
                if not is_generation and not is_exact_match:
                    self.data.append(DataEntry(question, answer, label == entry['answerKey'], 'yes'))
                    self.data.append(DataEntry(question, answer, label == entry['answerKey'], 'no'))
                else:
                    self.data.append(DataEntry(question, answer, label == entry['answerKey']))

        self.tokenizer = tokenizer
        self.max_len = max_len
        self.is_generation = is_generation
        self.is_exact_match = is_exact_match

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        entry = self.data[index]

        if self.is_generation:
            prompt = self.GENERATION_PROMPT
            if self.is_exact_match:
                prompt = prompt.replace('[ANSWER]', "")
            else:
                prompt = prompt.replace('[ANSWER]', entry.answer)
        else:
            prompt = self.DISCRIMINATION_PROMPT
            prompt = prompt.replace('[ANSWER]', entry.answer)

        prompt = prompt.replace('[QUESTION]', entry.question)

        # Will only do so when an output is provided.
        self.prompt = prompt.replace('[OUTPUT]', entry.output)

        input_ids = self.tokenizer(self.prompt, return_tensors="pt", truncation=True, padding='max_length',
                                   max_length=self.max_len).input_ids

        if entry.output == "":
            return input_ids, entry.answer, entry.is_correct, self.prompt
        else:
            return input_ids, entry.output, entry.is_correct, self.prompt


class DataEntry:
    """
    Input entry in the format, for e.g.,
    'answerKey': 'E',
    'question': 'A man is incarcerated in prison, and as his punishment he has to carry a one tonne bag of sand
    backwards and forwards across a field the size of a football pitch. What is the one thing he can put in it to
    make it lighter?',
    'choices':
            'label': ['A', 'B', 'C', 'D', 'E'],
            'text': ['throw', 'bit', 'gallon', 'mouse', 'hole']
    """

    def __init__(self, question, answer, is_correct, output=""):
        self.question = question.strip()
        self.answer = answer.strip()
        self.is_correct = is_correct
        self.output = output


def get_first_new_token(output, prompt):
    """Return the first nonempty token generated by the model (different from the prompt)."""
    split_prompt = prompt.split()
    split_output = output.split()

    index = len(split_prompt)
    while index < len(split_output):
        token = split_output[index]
        if token != "" and token is not None:
            return token, index
        index += 1

    return None, None


def normalise_log_probabilities(log_prob):
    # Convert to non-log probabilities.
    prob = np.exp(np.array(log_prob))
    # Normalise the probabilities.
    return [p / sum(prob) for p in prob]


def write_to_file(is_generation, is_exact_match, result):
    if is_generation:
        task = "generation"
    else:
        task = "discrimination"

    if is_exact_match:
        accuracy = "exactmatch"
    else:
        accuracy = "logprob"

    # Write results
    file = open(f"{os.path.dirname(os.path.abspath(__file__))}/results/{SIZE}_{task}_{accuracy}_batch_{BATCH_SIZE}.txt",
                "w")
    file.write(f"{accuracy} for {task} model {SIZE}")
    file.write("\n----------------------\n")
    file.write(f"{result}  | ")
    file.write("\n----------------------\n\n")

    file.close()


if __name__ == '__main__':
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    print(f"Using: {device}\n")

    is_generation = sys.argv[1] == '0'
    print(f"IS GENERATION: {is_generation}")
    is_exact_match = sys.argv[2] == '0'
    print(f"IS EXACT MATCH: {is_exact_match}")
    SIZE = sys.argv[3]  # 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B.
    print(f"MODEL SIZE: {SIZE}")
    BATCH_SIZE = int(sys.argv[4])  # Need to be multiple of 2 (yes/no) * 5 (possible answers).
    print(f"BATCH SIZE: {BATCH_SIZE}")

    RANDOM_SEED = 42
    K = 5
    DO_SAMPLE = K != 1
    MAX_SEQUENCE_LENGTH = 160
    REVISION = "step143000"
    MODEL_NAME = f'EleutherAI/pythia-{SIZE}-deduped'
    CACHE_DIR = f'{os.path.dirname(os.path.abspath(__file__))}/pythia-{SIZE}-deduped/{REVISION}'

    # Setting the random seed.
    random.seed(RANDOM_SEED)
    torch.manual_seed(RANDOM_SEED)
    set_seed(RANDOM_SEED)

    print("\n---- PREPARING DATASET ----")
    original_dataset = load_dataset('riddle_sense')

    # Split validation set into two.
    half = int(round(len(original_dataset["validation"]) / 2, 0))
    valid_dataset = list(islice(original_dataset["validation"], half))
    eval_dataset = list(islice(original_dataset["validation"], half, len(original_dataset["validation"])))

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        cache_dir=CACHE_DIR,
    )

    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'

    # A few pre-processing steps.
    valid_set = RiddleSenseDataset(eval_dataset, tokenizer, MAX_SEQUENCE_LENGTH,
                                   is_generation=is_generation, is_exact_match=is_exact_match)
    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE)
    total_entries = len(valid_set.data)
    number_of_batches = len(valid_loader)

    print("\n---- DOWNLOADING MODEL ----")
    model = GPTNeoXForCausalLM.from_pretrained(
        MODEL_NAME,
        cache_dir=CACHE_DIR,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )
    model.to(device)

    processed_batches = 0
    if is_generation:
        print(f"\n---- GENERATION for {SIZE}----\n")

        if is_exact_match:
            total_score = 0
            for input_ids, answers, labels, prompts in iter(valid_loader):
                processed_batches += 1
                if processed_batches % 10 == 0:
                    print(f'Processed {processed_batches}/{number_of_batches} batches.')

                input_ids = input_ids.squeeze(1).to(device)

                with torch.inference_mode():
                    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, num_return_sequences=K,
                                             do_sample=DO_SAMPLE, max_new_tokens=MAX_SEQUENCE_LENGTH + 10,
                                             use_cache=True)
                tokens = tokenizer.batch_decode(outputs, skip_special_tokens=True)

                index = 0
                for sample in range(len(answers)):
                    sample_score = 0
                    for k in range(K):
                        token, token_index = get_first_new_token(tokens[index], prompts[sample])
                        if token is not None:
                            answer_split = answers[sample].split(" ")

                            idx = 0
                            while idx < len(answer_split):
                                if token.lower() == answer_split[idx]:
                                    idx += 1
                                    token_index += 1
                                    if token_index < len(tokens[index]):
                                        token = tokens[index][token_index]
                                    else:
                                        break
                                else:
                                    break

                            if idx == len(answer_split):
                                sample_score += 1
                        index += 1
                    total_score += sample_score / K

            result = total_score / total_entries
            print(f"Exact match accuracy: {result}")
        else:
            sum_log_probabilities = 0
            for input_ids, answers, labels, prompts in iter(valid_loader):
                processed_batches += 1
                if processed_batches % 50 == 0:
                    print(f'Processed {processed_batches}/{number_of_batches} batches.')

                input_ids = input_ids.squeeze(1).to(device)
                with torch.inference_mode():
                    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, num_return_sequences=K,
                                             do_sample=DO_SAMPLE, max_new_tokens=MAX_SEQUENCE_LENGTH,
                                             use_cache=True, return_dict_in_generate=True, output_scores=True)
                gen_sequences = outputs.sequences[:, input_ids.shape[-1]:]
                scores = torch.stack(outputs.scores, dim=1).softmax(-1)
                gen_probs = torch.gather(scores, 2, gen_sequences[:, :, None]).squeeze(-1)

                probabilities = []
                for sample in range(len(answers)):
                    # Deal with multi-word/long tokens.
                    # Assumption: sum the probabilities of the parts.
                    # https://stackoverflow.com/questions/59435020/get-probability-of-multi-token-word-in-mask-position
                    answer = answers[sample]
                    log_prob = 0
                    token_index = len(input_ids[sample]) - 1
                    token = tokenizer.decode(input_ids[sample][token_index])
                    for k in range(K):
                        while token in answer:
                            print(token_index)
                            log_prob += gen_probs[k][token_index].item()
                            # Won't turn into an infinite loop since we put the answer at the end ourselves.
                            token_index -= 1
                            token = tokenizer.decode(input_ids[sample][token_index])
                    probabilities.append(log_prob / K)

                normalised_probabilities = []
                for i in range(0, len(answers), 5):
                    normalised_probabilities += normalise_log_probabilities(probabilities[i:i + 5])

                for sample in range(len(answers)):
                    if labels[sample]:
                        sum_log_probabilities += normalised_probabilities[sample]
                        break

            result = sum_log_probabilities / total_entries
            print(f"Log match accuracy: {result}")

    else:
        print(f"\n---- DISCRIMINATION for {SIZE}----\n")

        if is_exact_match:
            total_score = 0
            for input_ids, answers, labels, prompts in iter(valid_loader):
                processed_batches += 1
                if processed_batches % 10 == 0:
                    print(f'Processed {processed_batches}/{number_of_batches} batches.')

                input_ids = input_ids.squeeze(1).to(device)

                with torch.inference_mode():
                    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, num_return_sequences=K,
                                             do_sample=DO_SAMPLE, max_new_tokens=MAX_SEQUENCE_LENGTH + 10,
                                             use_cache=True)
                tokens = tokenizer.batch_decode(outputs, skip_special_tokens=True)

                index = 0
                for sample in range(len(answers)):
                    sample_score = 0
                    for k in range(K):
                        token, _ = get_first_new_token(tokens[index], prompts[sample])
                        if token is not None:
                            if (token.lower() == 'yes' and labels[sample]) or \
                                    (token.lower() == 'no' and not labels[sample]):
                                sample_score += 1
                        index += 1
                    total_score += sample_score / K

            result = total_score / total_entries
            print(f"Exact match accuracy: {result}")

        else:
            sum_log_probabilities = 0
            for input_ids, answers, labels, prompts in iter(valid_loader):
                processed_batches += 1
                if processed_batches % 50 == 0:
                    print(f'Processed {processed_batches}/{number_of_batches} batches.')

                input_ids = input_ids.squeeze(1).to(device)

                with torch.inference_mode():
                    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, num_return_sequences=K,
                                             do_sample=DO_SAMPLE, max_new_tokens=MAX_SEQUENCE_LENGTH,
                                             use_cache=True, return_dict_in_generate=True, output_scores=True)
                gen_sequences = outputs.sequences[:, input_ids.shape[-1]:]
                scores = torch.stack(outputs.scores, dim=1).softmax(-1)
                gen_probs = torch.gather(scores, 2, gen_sequences[:, :, None]).squeeze(-1)

                probabilities = []
                for sample in range(len(answers)):
                    log_prob = 0
                    # The answer yes/no is always the last token and won't be split.
                    token_index = len(input_ids[sample]) - 1
                    for k in range(K):
                        log_prob += gen_probs[k][token_index].item()
                    probabilities.append(log_prob / K)

                normalised_probabilities = []
                for i in range(0, len(answers), 2):
                    normalised_probabilities += normalise_log_probabilities(probabilities[i:i + 2])

                for sample in range(0, len(answers), 2):
                    if labels[sample]:
                        sum_log_probabilities += normalised_probabilities[sample]
                    else:
                        sum_log_probabilities += normalised_probabilities[sample + 1]

            result = sum_log_probabilities / total_entries
            print(f"Log match accuracy: {result}")

    write_to_file(is_generation, is_exact_match, result)
