from transformers import GPTNeoXForCausalLM, AutoTokenizer
import torch
import numpy as np
import random

from RiddleSenseDataset import RiddleSenseDataset

SEED = 42
DISCRIMINATION_PROMPT = "Q: [QUESTION]\nA: [ANSWER]\nIs this a correct answer to the riddle?\n[OUTPUT]"
GENERATION_PROMPT = "Q: [QUESTION]\nThe answer to the riddle is: [ANSWER]"
K = 5  # Sample size.


def get_first_new_token(output, prompt):
    """Return the first nonempty token generated by the model (different from the prompt)."""
    split_prompt = prompt.split()
    split_output = output.split()

    index = len(split_prompt)
    while index < len(split_output):
        token = split_output[index]
        if token != "" and token is not None:
            return token
        index += 1

    return None


def get_discrimination_accuracy(model, tokenizer, dataset):
    correct_outputs = 0
    total_outputs = 0

    for entry in dataset.entries['validation']:
        question = entry.question

        for label in entry.choices.keys():
            total_outputs += 1
            if total_outputs % 100 == 0:
                print(f'Processed {total_outputs} data entries.')

            answer = entry.choices[label]

            # exact match
            prompt = DISCRIMINATION_PROMPT \
                .replace('[QUESTION]', question) \
                .replace('[ANSWER]', answer) \
                .replace('[OUTPUT]', "")

            inputs = tokenizer(prompt, return_tensors="pt")
            tokens = model.generate(**inputs, max_length=160, pad_token_id=tokenizer.eos_token_id,
                                    num_return_sequences=K, do_sample=True)

            for k in range(K):
                token = get_first_new_token(tokenizer.decode(tokens[k]), prompt)
                if token is not None:
                    if 'yes' == token.lower():
                        if label == entry.answer_key:
                            correct_outputs += 1
                    # Anything else than yes is considered to be a "no".
                    elif not label == entry.answer_key:
                        correct_outputs += 1

    return correct_outputs / (total_outputs * K)


def get_generation_accuracy(model, tokenizer, dataset):
    correct_outputs = 0
    total_outputs = 0

    for entry in dataset.entries['validation']:
        total_outputs += 1
        if total_outputs % 100 == 0:
            print(f'Processed {total_outputs} data entries.')

        question = entry.question
        answer = entry.choices[entry.answer_key]

        # exact match
        prompt = GENERATION_PROMPT \
            .replace('[QUESTION]', question) \
            .replace('[ANSWER]', "")

        inputs = tokenizer(prompt, return_tensors="pt")
        tokens = model.generate(**inputs, max_length=160, pad_token_id=tokenizer.eos_token_id,
                                num_return_sequences=K, do_sample=True)

        for k in range(K):
            token = get_first_new_token(tokenizer.decode(tokens[k]), prompt)
            if token is not None:
                # For multi-word expressions, we consider that if the output contains part of the answer, it is correct.
                if token.lower() in answer:
                    correct_outputs += 1

    return correct_outputs / (total_outputs * K)


# TODO. Add averaging over k=5 samples.
def get_generation_log_accuracy(model, tokenizer, dataset):
    sum_log_probabilities = 0
    total_outputs = 0

    for entry in dataset.entries['validation']:
        total_outputs += 1
        if total_outputs % 100 == 0:
            print(f'Processed {total_outputs} data entries.')

        question = entry.question
        probabilities = []
        labels = []

        for label in entry.choices.keys():
            answer = entry.choices[label]

            prompt = GENERATION_PROMPT \
                .replace('[QUESTION]', question) \
                .replace('[ANSWER]', answer)

            input_ids = tokenizer(prompt, padding=True, return_tensors="pt").input_ids
            outputs = model(input_ids)
            probs = torch.log_softmax(outputs.logits, dim=-1).detach()

            # Collect the probability of the generated token:
            # probability at index 0 corresponds to the token at index 1.
            probs = probs[:, :-1, :]
            input_ids = input_ids[:, 1:]
            gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)

            log_prob = 0
            for token, p in zip(input_ids[0], gen_probs[0]):
                if token not in tokenizer.all_special_ids:
                    # Deal with multi-word/long tokens.
                    # Assumption: sum the probabilities of the parts.
                    # https://stackoverflow.com/questions/59435020/get-probability-of-multi-token-word-in-mask-position
                    if tokenizer.decode(token) in answer:
                        log_prob += p.item()
            probabilities.append(log_prob)
            labels.append(label)

        # Convert to non-log probabilities.
        probabilities = np.exp(np.array(probabilities))
        # Normalise the probabilities.
        normalised_probabilities = [p / sum(probabilities) for p in probabilities]

        for i in range(len(labels)):
            if labels[i] == entry.answer_key:
                sum_log_probabilities += normalised_probabilities[i]
                break

    return sum_log_probabilities / total_outputs


def get_discrimination_log_accuracy(model, tokenizer, dataset):
    sum_log_probabilities = 0
    total_outputs = 0

    for entry in dataset.entries['validation']:
        question = entry.question

        for label in entry.choices.keys():
            total_outputs += 1
            if total_outputs % 100 == 0:
                print(f'Processed {total_outputs} data entries.')

            answer = entry.choices[label]
            probabilities = []

            for output in ["Yes", "No"]:
                prompt = DISCRIMINATION_PROMPT \
                    .replace('[QUESTION]', question) \
                    .replace('[ANSWER]', answer) \
                    .replace('[OUTPUT]', output)

                input_ids = tokenizer(prompt, padding=True, return_tensors="pt").input_ids
                outputs = model(input_ids)
                probs = torch.log_softmax(outputs.logits, dim=-1).detach()

                # Collect the probability of the generated token:
                # probability at index 0 corresponds to the token at index 1.
                probs = probs[:, :-1, :]
                input_ids = input_ids[:, 1:]
                gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)

                # Deal with multi-word/long tokens.
                # Assumption: sum the probabilities of the parts.
                # https://stackoverflow.com/questions/59435020/get-probability-of-multi-token-word-in-mask-position
                log_prob = 0
                for token, p in zip(input_ids[0], gen_probs[0]):
                    if token not in tokenizer.all_special_ids:
                        if tokenizer.decode(token) in output:
                            log_prob += p.item()
                probabilities.append(log_prob)

            # Convert to non-log probabilities.
            probabilities = np.exp(np.array(probabilities))
            # Normalise the probabilities.
            normalised_probabilities = [p / sum(probabilities) for p in probabilities]

            if label == entry.answer_key:
                sum_log_probabilities += normalised_probabilities[0]
            else:
                sum_log_probabilities += normalised_probabilities[1]

    return sum_log_probabilities / total_outputs


if __name__ == '__main__':
    # Settings
    SEED = 42
    random.seed(SEED)
    MODEL_NAME = 'EleutherAI/pythia-70m-deduped'
    REVISION = 'step3000'
    CACHE_DIR = './pythia-70m-deduped/step3000'

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        revision=REVISION,
        cache_dir=CACHE_DIR,
    )

    tokenizer.pad_token = tokenizer.eos_token

    # Vary model sizes:
    # 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B.
    model = GPTNeoXForCausalLM.from_pretrained(
        MODEL_NAME,
        revision=REVISION,
        cache_dir=CACHE_DIR
    )

    # TODO. Pre-process dataset.
    dataset = RiddleSenseDataset()

    g_acc = get_generation_accuracy(model, tokenizer, dataset)
    print(g_acc)
